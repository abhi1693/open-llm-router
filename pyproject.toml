[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "open-llm-router"
version = "0.1.0"
description = "OpenAI-compatible Open LLM router for self-hosted model backends"
requires-python = ">=3.12"
dependencies = [
    "fastapi>=0.115.0",
    "httpx>=0.28.0",
    "pyjwt[crypto]>=2.10.1",
    "pydantic-settings>=2.6.0",
    "pyyaml>=6.0.2",
    "uvicorn>=0.32.0",
    "transformers>=5.2.0",
    "torch>=2.10.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.3.0",
    "black>=24.3.0",
    "flake8>=7.1.0",
    "isort>=5.13.2",
    "ruff>=0.5.0",
]

[project.scripts]
open-llm-router = "open_llm_router.main:run"
router = "open_llm_router.router_cli:main"
router-log-analyze = "open_llm_router.log_analysis_cli:main"

[tool.pytest.ini_options]
pythonpath = ["."]

[tool.isort]
profile = "black"

[tool.mypy]
strict = true
files = ["open_llm_router", "tests"]
python_version = "3.12"

[tool.uv]
package = true

[tool.setuptools]
include-package-data = true

[tool.setuptools.packages.find]
include = ["open_llm_router*"]
exclude = ["config*"]

[tool.setuptools.package-data]
open_llm_router = ["catalog/*.yaml"]

[dependency-groups]
dev = [
    "h2>=4.3.0",
    "mypy>=1.19.1",
    "opentelemetry-api>=1.39.1",
    "opentelemetry-exporter-otlp-proto-http>=1.39.1",
    "opentelemetry-instrumentation-fastapi>=0.60b1",
    "opentelemetry-instrumentation-httpx>=0.60b1",
    "opentelemetry-sdk>=1.39.1",
    "redis>=7.2.0",
    "types-pyyaml>=6.0.12.20250915",
]
