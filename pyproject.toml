[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "open-llm-router"
version = "0.1.0"
description = "OpenAI-compatible Open LLM router for self-hosted model backends"
requires-python = ">=3.12"
dependencies = [
    "fastapi>=0.115.0",
    "httpx>=0.28.0",
    "pyjwt[crypto]>=2.10.1",
    "pydantic-settings>=2.6.0",
    "pyyaml>=6.0.2",
    "uvicorn>=0.32.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.3.0",
    "black>=24.3.0",
    "flake8>=7.1.0",
    "isort>=5.13.2",
    "ruff>=0.5.0",
]

[project.scripts]
open-llm-router = "open_llm_router.main:run"
router = "open_llm_router.router_cli:main"

[tool.pytest.ini_options]
pythonpath = ["."]

[tool.uv]
package = true

[tool.setuptools]
include-package-data = true

[tool.setuptools.packages.find]
include = ["open_llm_router*"]
exclude = ["config*"]

[tool.setuptools.package-data]
open_llm_router = ["catalog/*.yaml"]
